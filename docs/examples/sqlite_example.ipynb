{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5Bx7tBafUEP"
      },
      "source": [
        "# Getting Started With Evalbench\n",
        "\n",
        "EvalBench is a flexible framework designed to measure the quality of generative AI (GenAI) workflows around database specific tasks. As of now, it provides a comprehensive set of tools, and modules to evaluate models on NL2SQL tasks, including capability of running and scoring DQL, DML, and DDL queries across multiple supported databases. Its modular, plug-and-play architecture allows you to seamlessly integrate custom components while leveraging a robust evaluation pipeline, result storage, scoring strategies, and dashboarding capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GKI9_72faY9"
      },
      "source": [
        "### Quick Start Example w/ sqllite\n",
        "#### 1. Clone the EvalBench repository from GitHub:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVZAQXctfFnx"
      },
      "outputs": [],
      "source": [
        "!git clone git@github.com:GoogleCloudPlatform/evalbench.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7IDm5ZffnqH"
      },
      "source": [
        "#### 2. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L37hxmGLf-wM"
      },
      "outputs": [],
      "source": [
        "!pip install -r evalbench/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDoc62MDz5UO"
      },
      "source": [
        "#### 3. Setup Evalbench environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8VSHGEG0Bmn"
      },
      "outputs": [],
      "source": [
        "cd evalbench"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym5qP0zfhDnf"
      },
      "source": [
        "#### 4. Connect with GCP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3_MT1r_z-mR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['EVAL_GCP_PROJECT_ID'] = '<put-your-project-id-here>'\n",
        "os.environ['EVAL_GCP_PROJECT_REGION'] = '<gcp-region-here>'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "AwQBxt_a0G04"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user(project_id=os.environ['EVAL_GCP_PROJECT_ID'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjT5t3vm10kQ"
      },
      "source": [
        "#### 5. Run Evalbench"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCPUOdUkhGiE"
      },
      "outputs": [],
      "source": [
        "!python3 evalbench/evalbench.py  --experiment_config=\"datasets/bat/db_blog/run_configs/sqlite/run_dql.yaml\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrPvYYX817yt"
      },
      "source": [
        "#### 6. Build a Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMEmWAdI2IG-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "results_dir = \"results/\"\n",
        "\n",
        "# Find the first folder in the results directory\n",
        "# NOTE: Change this logic if you have a specific job_id you want to find\n",
        "first_folder = None\n",
        "for folder in os.listdir(results_dir):\n",
        "  folder_path = os.path.join(results_dir, folder)\n",
        "  if not folder.startswith(\".\") and os.path.isdir(folder_path):\n",
        "    first_folder = folder_path\n",
        "    break\n",
        "\n",
        "if first_folder:\n",
        "  summary_file = os.path.join(first_folder, \"summary.csv\")\n",
        "  if os.path.exists(summary_file):\n",
        "    df = pd.read_csv(summary_file)\n",
        "    df['percentage'] = (df['correct_results_count'] / df['total_results_count']) * 100\n",
        "    df_sorted = df.sort_values(by='percentage', ascending=False)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    bars = plt.bar(df_sorted['metric_name'], df_sorted['percentage'], color='skyblue')\n",
        "    plt.xlabel('Metric Name')\n",
        "    plt.ylabel('Correct Results (%)')\n",
        "    plt.title('Percentage of Correct Results per Metric')\n",
        "    plt.ylim(0, 110)\n",
        "    plt.xticks(rotation=45)\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width() / 2, height + 1, f'{height:.1f}%',\n",
        "                ha='center', va='bottom', fontsize=9)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "  else:\n",
        "    print(f\"summary.csv not found in {os.path.join(first_folder, 'summary.csv')}.\")\n",
        "else:\n",
        "  print(\"No results found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u41cgkZQc8QS"
      },
      "source": [
        "Now report on the overall evaluations and their scoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWUh0Mzfc7IH"
      },
      "outputs": [],
      "source": [
        "from google.colab import data_table\n",
        "\n",
        "evals_file = os.path.join(first_folder, \"evals.csv\")\n",
        "scores_file = os.path.join(first_folder, \"scores.csv\")\n",
        "if not os.path.exists(scores_file) or not os.path.exists(evals_file):\n",
        "  print(\"No results found.\")\n",
        "  exit()\n",
        "\n",
        "evals_df = pd.read_csv(evals_file)\n",
        "scores_df = pd.read_csv(scores_file)\n",
        "scores_pivot = scores_df.pivot_table(\n",
        "    index=[\"id\", \"job_id\"],\n",
        "    columns=\"comparator\",\n",
        "    values=\"score\",\n",
        "    aggfunc=\"first\"\n",
        ").reset_index()\n",
        "scores_pivot.columns.name = None\n",
        "scores_pivot = scores_pivot.rename(columns={\n",
        "    \"returned_sql\": \"score_returned_sql\",\n",
        "    \"llmrater\": \"score_llmrater\",\n",
        "    \"set_match\": \"score_set_match\",\n",
        "    \"exact_match\": \"score_exact_match\"\n",
        "})\n",
        "merged_df = pd.merge(evals_df, scores_pivot, on=[\"id\", \"job_id\"], how=\"left\")\n",
        "merged_df[\"score_executable\"] = merged_df[\"generated_error\"].isna().astype(int) * 100\n",
        "final_df = merged_df[[\n",
        "    \"id\",\n",
        "    \"nl_prompt\",\n",
        "    \"generated_sql\",\n",
        "    \"golden_sql\",\n",
        "    \"generated_result\",\n",
        "    \"golden_result\",\n",
        "    \"score_returned_sql\",\n",
        "    \"score_executable\",\n",
        "    \"score_llmrater\",\n",
        "    \"score_set_match\",\n",
        "    \"score_exact_match\"\n",
        "]].rename(columns={\n",
        "    \"generated_sql\": \"generated_query\",\n",
        "    \"golden_sql\": \"golden_query\"\n",
        "})\n",
        "data_table.enable_dataframe_formatter()\n",
        "final_df"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
