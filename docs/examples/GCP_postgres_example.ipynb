{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5Bx7tBafUEP"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/GoogleCloudPlatform/evalbench/blob/main/docs/examples/sqlite_example.ipynb)\n",
        "\n",
        "# Getting Started With Evalbench\n",
        "\n",
        "EvalBench is a flexible framework designed to measure the quality of generative AI (GenAI) workflows around database specific tasks. As of now, it provides a comprehensive set of tools, and modules to evaluate models on NL2SQL tasks, including capability of running and scoring DQL, DML, and DDL queries across multiple supported databases. Its modular, plug-and-play architecture allows you to seamlessly integrate custom components while leveraging a robust evaluation pipeline, result storage, scoring strategies, and dashboarding capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GKI9_72faY9"
      },
      "source": [
        "### Quick Start Example w/ GCP Postgres\n",
        "#### 1. Clone the EvalBench repository from GitHub:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFazUzuzTDvh"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/GoogleCloudPlatform/evalbench.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7IDm5ZffnqH"
      },
      "source": [
        "#### 2. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9QfnrcQTOTR"
      },
      "outputs": [],
      "source": [
        "!pip install -r evalbench/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDoc62MDz5UO"
      },
      "source": [
        "#### 3. Setup Evalbench environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8VSHGEG0Bmn"
      },
      "outputs": [],
      "source": [
        "cd evalbench"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym5qP0zfhDnf"
      },
      "source": [
        "#### 4. Connect with GCP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL7JIIeEQ1Zz"
      },
      "source": [
        "**NOTE: Update Your GCP Project ID and Region Below.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3_MT1r_z-mR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['EVAL_GCP_PROJECT_ID'] = ''\n",
        "os.environ['EVAL_GCP_PROJECT_REGION'] = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwQBxt_a0G04"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user(project_id=os.environ['EVAL_GCP_PROJECT_ID'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3v0MsBsT5n0"
      },
      "source": [
        "#### 5. Update the run config file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kzw7aqTLeXxJ"
      },
      "outputs": [],
      "source": [
        "run_config = \"\"\"############################################################\n",
        "### Dataset / Eval Items\n",
        "############################################################\n",
        "# The JSON list of prompts / golden SQLs and eval attributes for the run\n",
        "dataset_config: datasets/bat/prompts.json\n",
        "### Database Info\n",
        "# The YAML config for the database connection information\n",
        "database_configs:\n",
        " - datasets/bat/db_configs/postgres.yaml\n",
        "# The dialect that the dataset_config will be filtered by. Only one can be\n",
        "# specified at a time (per run). See above for list of supported Dialects\n",
        "dialects:\n",
        " - postgres\n",
        "query_types:\n",
        " - dql\n",
        "#\n",
        "#\n",
        "############################################################\n",
        "### Prompt and Generation Modules\n",
        "############################################################\n",
        "# The YAML config for the model to be used for generation.\n",
        "model_config: datasets/bat/model_configs/gemini_2.5_pro_model.yaml\n",
        "# The prompt generator module id for prompt generation.\n",
        "prompt_generator: 'SQLGenBasePromptGenerator'\n",
        "#\n",
        "#\n",
        "############################################################\n",
        "### Optional - Setup / Teardown related configs (Required for testing DDL)\n",
        "############################################################\n",
        "# Used for setup / teardown, the directory path to the sql files used for setting up\n",
        "# / tearing down a database instance. This is required for running DDL\n",
        "setup_directory: datasets/bat/setup\n",
        "#\n",
        "#\n",
        "############################################################\n",
        "### Scorer Related Configs - See /scorers directory for each scorer.\n",
        "############################################################\n",
        "scorers:\n",
        "  exact_match: null\n",
        "  llmrater:\n",
        "    model_config: datasets/bat/model_configs/gemini_1.5-pro-002_model.yaml\n",
        "  returned_sql: null\n",
        "  set_match: null\n",
        "  executable_sql: null\n",
        "#\n",
        "#\n",
        "############################################################\n",
        "### Reporting Related Configs\n",
        "############################################################\n",
        "reporting:\n",
        "  csv:\n",
        "    output_directory: 'results'\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCaJhVW5VuyK"
      },
      "outputs": [],
      "source": [
        "!echo \"{run_config}\" > datasets/bat/example_run_config.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxhh9KeEe6BF"
      },
      "source": [
        "#### 6. Update the database config file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**NOTE: Update Your DB name, DB path, username and password Below.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWNuMRkle_RC"
      },
      "outputs": [],
      "source": [
        "db_config = \"\"\"db_type: postgres\n",
        "database_name: <your-db-name>\n",
        "database_path: <your-db-path>\n",
        "max_executions_per_minute: 180\n",
        "user_name: <your-username>\n",
        "password: <your-password>\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4H0JZEYfHQ4"
      },
      "outputs": [],
      "source": [
        "!echo \"{db_config}\" > datasets/bat/db_configs/postgres.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjT5t3vm10kQ"
      },
      "source": [
        "#### 7. Run Evalbench"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFvcdn0AT1pt"
      },
      "outputs": [],
      "source": [
        "%run evalbench/evalbench.py  --experiment_config=\"datasets/bat/example_run_config.yaml\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrPvYYX817yt"
      },
      "source": [
        "#### 8. Build a Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMEmWAdI2IG-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "results_dir = \"results/\"\n",
        "\n",
        "# Find the first folder in the results directory\n",
        "# NOTE: Change this logic if you have a specific job_id you want to find\n",
        "first_folder = None\n",
        "for folder in os.listdir(results_dir):\n",
        "  folder_path = os.path.join(results_dir, folder)\n",
        "  if not folder.startswith(\".\") and os.path.isdir(folder_path):\n",
        "    first_folder = folder_path\n",
        "    break\n",
        "\n",
        "if first_folder:\n",
        "  summary_file = os.path.join(first_folder, \"summary.csv\")\n",
        "  if os.path.exists(summary_file):\n",
        "    df = pd.read_csv(summary_file)\n",
        "    df['percentage'] = (df['correct_results_count'] / df['total_results_count']) * 100\n",
        "    df_sorted = df.sort_values(by='percentage', ascending=False)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    bars = plt.bar(df_sorted['metric_name'], df_sorted['percentage'], color='skyblue')\n",
        "    plt.xlabel('Metric Name')\n",
        "    plt.ylabel('Correct Results (%)')\n",
        "    plt.title('Percentage of Correct Results per Metric')\n",
        "    plt.ylim(0, 110)\n",
        "    plt.xticks(rotation=45)\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width() / 2, height + 1, f'{height:.1f}%',\n",
        "                ha='center', va='bottom', fontsize=9)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "  else:\n",
        "    print(f\"summary.csv not found in {os.path.join(first_folder, 'summary.csv')}.\")\n",
        "else:\n",
        "  print(\"No results found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u41cgkZQc8QS"
      },
      "source": [
        "Now report on the overall evaluations and their scoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWUh0Mzfc7IH"
      },
      "outputs": [],
      "source": [
        "from google.colab import data_table\n",
        "\n",
        "evals_file = os.path.join(first_folder, \"evals.csv\")\n",
        "scores_file = os.path.join(first_folder, \"scores.csv\")\n",
        "if not os.path.exists(scores_file) or not os.path.exists(evals_file):\n",
        "  print(\"No results found.\")\n",
        "  exit()\n",
        "\n",
        "evals_df = pd.read_csv(evals_file)\n",
        "scores_df = pd.read_csv(scores_file)\n",
        "scores_pivot = scores_df.pivot_table(\n",
        "    index=[\"id\", \"job_id\"],\n",
        "    columns=\"comparator\",\n",
        "    values=\"score\",\n",
        "    aggfunc=\"first\"\n",
        ").reset_index()\n",
        "scores_pivot.columns.name = None\n",
        "scores_pivot = scores_pivot.rename(columns={\n",
        "    \"returned_sql\": \"score_returned_sql\",\n",
        "    \"llmrater\": \"score_llmrater\",\n",
        "    \"set_match\": \"score_set_match\",\n",
        "    \"exact_match\": \"score_exact_match\"\n",
        "})\n",
        "merged_df = pd.merge(evals_df, scores_pivot, on=[\"id\", \"job_id\"], how=\"left\")\n",
        "merged_df[\"score_executable\"] = merged_df[\"generated_error\"].isna().astype(int) * 100\n",
        "final_df = merged_df[[\n",
        "    \"id\",\n",
        "    \"nl_prompt\",\n",
        "    \"generated_sql\",\n",
        "    \"golden_sql\",\n",
        "    \"generated_result\",\n",
        "    \"golden_result\",\n",
        "    \"score_returned_sql\",\n",
        "    \"score_executable\",\n",
        "    \"score_llmrater\",\n",
        "    \"score_set_match\",\n",
        "    \"score_exact_match\"\n",
        "]].rename(columns={\n",
        "    \"generated_sql\": \"generated_query\",\n",
        "    \"golden_sql\": \"golden_query\"\n",
        "})\n",
        "data_table.enable_dataframe_formatter()\n",
        "final_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_xrwLWzQ1Zz"
      },
      "source": [
        "Congrats! You have done it!! Please refer to the [EvalBench documentation](https://github.com/GoogleCloudPlatform/evalbench) for additional information including how to configure more complicated [run-configs](https://github.com/GoogleCloudPlatform/evalbench/blob/main/docs/configs/run-config.md). Enjoy evaluating your GenAI models!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
