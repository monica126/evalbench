"""
RecallMatcher

This is a straightforward scoring strategy which calculates the precision and recall from generated and expected results ignoring the None and duplicate values.

precision = no. of matching results / no. of expected results
recall = no. of matching results / no. of generated results

Note: This strategy is only concerned with number of correct results, ordering of the results do not matter.

Run configurations:
   1. score_type: Optional
       - Accepted values: "precision", "recall"
       - Default value: "recall"
       - Comparator returns precision or recall as the score according to this option
"""

from typing import Tuple

from scorers.comparator import Comparator


class RecallMatcher(Comparator):
    """
    RecallMatcher class implements the Comparator base class with precision and recall calculation logic.

    Attributes:
        1. name: Name of the comparator. Set to "recall_match"
        2. config: Scorer config defined in the run config yaml file
        3. score_type: one out of precision and recall which will be used as final score
    """

    def __init__(self, config: dict):
        self.name = "recall_match"
        self.config = config
        self.score_type = "recall"

        if config and "score_type" in config:
            self.score_type = config["score_type"]

    def find_common_dicts(self, list1, list2):
        """
        Finds common dictionaries between two lists of dictionaries.
        Two dictionaries are considered the same if all their key-value pairs are the same.
        Dictionaries can contain lists as values.

        Args:
            1. list1: The first list of dictionaries.
            2. list2: The second list of dictionaries.

        Returns:
            1. A list of common dictionaries.
            2. No. of unique items in list1
            3. No. of unique items in list2
        """

        def make_hashable(item):
            """Recursively converts items to hashable types (tuples)."""
            if isinstance(item, list):
                return tuple(make_hashable(x) for x in item)
            elif isinstance(item, dict):
                return tuple(sorted((k, make_hashable(v)) for k, v in item.items()))
            else:
                return item

        set1 = {make_hashable(d) for d in list1}
        set2 = {make_hashable(d) for d in list2}

        common_hashables = set1.intersection(set2)

        # Preserve original dictionaries
        common_dicts = [
            d for d in list1 if make_hashable(d) in common_hashables
        ]
        return common_dicts, len(set1), len(set2)

    def compute_precision_recall(self, golden_results, generated_results):
        """
        Calculates precision and recall for two sets of results, removing duplicates and Nones.

        Args:
            golden_results: A list of the correct results.
            generated_results: A list of the results generated by the model.

        Returns:
            A dictionary containing the following:
                1. precision
                2. recall
                3. orig_golden_size: no. of expected results before removing duplicates
                4. orig_generated_size: no. of generated results before removing duplicates
                5. dedup_golden_size: no. of expected results after removing duplicates
                6. dedup_generated_size: no. of generated results after removing duplicates
       """

        # Filter out None values (assuming they shouldn't be considered)
        golden_results = (
            [x for x in golden_results if x is not None] if golden_results else []
        )
        generated_results = (
            [x for x in generated_results if x is not None] if generated_results else []
        )

        orig_golden_size = len(golden_results)
        orig_generated_size = len(generated_results)

        common_results, dedup_golden_size, dedup_generated_size = self.find_common_dicts(golden_results, generated_results)
        correct_predictions = len(common_results)

        if golden_results == generated_results:
            precision = 1
            recall = 1
        else:
            # Calculate precision and recall
            recall = correct_predictions / len(generated_results) if generated_results else 0
            precision = correct_predictions / len(golden_results) if golden_results else 0

        full_result = {
            "precision": precision,
            "recall": recall,
            "orig_golden_size": orig_golden_size,
            "orig_generated_size": orig_generated_size,
            "dedup_golden_size": dedup_golden_size,
            "dedup_generated_size": dedup_generated_size,
        }
        return full_result

    def compare(
        self,
        nl_prompt: str,
        golden_query: str,
        query_type: str,
        golden_execution_result: list[str],
        golden_eval_result: str,
        golden_error: str,
        generated_query: str,
        generated_execution_result: list[str],
        generated_eval_result: str,
        generated_error: str,
    ) -> Tuple[float, str]:
        """Implements the scoring logic by using precision or recall."""

        full_result = self.compute_precision_recall(
            golden_execution_result, generated_execution_result
        )
        return full_result[self.score_type]*100, str(full_result)
